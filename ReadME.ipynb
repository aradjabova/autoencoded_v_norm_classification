{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReadME.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HzW_L1OEejMrRGPi7aoO5kfWjYK0Z7lc",
      "authorship_tag": "ABX9TyO1Ie4JlA324fOqUWC0ck6f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aradjabova/dsc-capstone-project-v2-online-ds-ft-120919/blob/master/ReadME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB7tueKTpjw0",
        "colab_type": "text"
      },
      "source": [
        "# Experiment: Autoencoded Images  Vs. Original Images for Classification\n",
        "\n",
        "Created a trial code with MNIST dataset and then used the kaggle intel image classificaiton dataset for experiment.\n",
        "https://www.kaggle.com/puneet6060/intel-image-classification/version/2#20060.jpg \n",
        "\n",
        "\n",
        "### Experiment: Do augmented images have better image classification results?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B6nFOnEUmN6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Trial dataset MNIST\n",
        "\n",
        "<details>\n",
        "  <summary>Click to read about the trail dataset! </summary>\n",
        "  \n",
        "  ## Obtaining\n",
        "  * We have gathered the data from https://keras.io/api/datasets/mnist/\n",
        "  * This is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n",
        "    \n",
        "\n",
        "<img  src=\"https://drive.google.com/uc?id=1m58DO3twhgExsLLVtayU4MH2Rzzb3XCT\">\n",
        "\n",
        "\n",
        "  * Fourtunately, the dataset has plenty of images and there was no cleaning, creating required.\n",
        "\n",
        "Using the OSEM process we will build an autoencoder and an image classifier.\n",
        "\n",
        "### Autoencoder\n",
        "  * Is used to condense the information in the images\n",
        "  * Condensing the information allows the autoencoder learn the most important parts of an image\n",
        "  * Once the images are condensed the decoder will use the learned information to expand the images to the original size\n",
        "\n",
        "### Image Classification\n",
        " * Is created to teach the machine to predict what the image is showing\n",
        "\n",
        "This is an unbalanced multi-class classification problem; thought because we have plenty of each image, we will not need to create more images.\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary>Click to read about the trail dataset autoencoded! </summary>\n",
        "  \n",
        "  ## Autoencoder\n",
        "    \n",
        "#### Standard Autoencoder\n",
        "<img  src=\"https://drive.google.com/uc?id=1DR6ccJJwfJOz4rJuYjj9v5mZ4-UCsP6Q\">\n",
        "\n",
        "  * Here you can see that the original images (top) that are inputed are condensed and then reblown into blurry images of it self (bottom).\n",
        "\n",
        "\n",
        "#### Noisy Autoencoder\n",
        "<img  src=\"https://drive.google.com/uc?id=1b6YiWQBs7G2L83gpIdY5g5GQPD04E_Ki\">\n",
        "\n",
        "  * Here we can see that the \"noisy\" input (bottom), meaning having additional pixels added, outputs clean images simlar to the original images\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "  <summary>Click to read about the trail dataset image classification! </summary>\n",
        "  \n",
        "  ## Classification\n",
        "    \n",
        "#### Simple CNN (Central Neural Network) for Image Classification\n",
        "\n",
        "When creating a simple image classification there are different methods of making sure how well the machine is learning. \n",
        "\n",
        "<img  src=\"https://drive.google.com/uc?id=1Phv01zz-TsENuX5xDUz31r9s5dW6GvU7\">\n",
        "<img  src=\"https://drive.google.com/uc?id=1r7w9cMA2P5cbNLzT__JaaZ1mgusWFTQN\">\n",
        "\n",
        "  * These show how well the model has been learning. The higher the accuracy during training and validation dictates better results\n",
        "  * Lower loss shows that the amount of error is decreasing\n",
        "\n",
        "#### Confusion Matrix\n",
        "\n",
        "<img  src=\"https://drive.google.com/uc?id=1SVQ0WGCXWRKc-4RhvV0t4LlBFgfXi9_f\" align=\"middle\">\n",
        "\n",
        "  * These are a great visual representation of how well the model is performing and learning.\n",
        "  * The diagnoal line of the confusion matrix indicates the accurately labeled images.\n",
        "\n",
        "#### Classification Report\n",
        "<img  src=\"https://drive.google.com/uc?id=1reDBGIK1_qEi3qW9D0V-ePj3k0QzqgfC\" >\n",
        "\n",
        "* Another way of seeing how well the images are being identified.\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "##### Conclusions:\n",
        "\n",
        "\n",
        "The MNIST dataset is used in order to complete the trial code. The evaluations of the models are a good indication of the experiment goal results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkZKsGlLUl6u",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Control\n",
        "<details>\n",
        "  <summary>Click to read about modeling the control data! </summary>\n",
        "  \n",
        "  ## Dataset\n",
        "  The kaggle data is a dataset of images of building, forest, glacier, mountain, sea and street.\n",
        "\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1tpWgffkIFDqwUr13JlCK8UrgTHmZAjoz\" >\n",
        "\n",
        "There is a decent amount of images available for each category, but it may impact the amount that can be learned from the current amount of images.\n",
        "\n",
        "</details>\n",
        "\n",
        "  <details>\n",
        "    <summary>Click to view examples of the images in the dataset\n",
        "    </summary>\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1uW7i_tVnzfgaX2BxNgtGIGPxc95Xvpe5\" width=\"400\" height=\"200\" >\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1EXl96CvTaTP0xOrmMunCnUVtOt6Ok3d5\" width=\"400\" height=\"200\" >\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1w3WeUWl9PEYkyzdKFXdoiHouYo4IE0Bk\" width=\"400\" height=\"200\" >\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1OjqQwdxPKO6cLeVY1hlUOu_yThE27IEW\" width=\"400\" height=\"200\" >\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1mVKzKjt64HxZiC-O534MsusmoUOcQOnY\" width=\"400\" height=\"200\">\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1BUKUx4cTK0GS92dawf2nCkSilncdvM-R\" width=\"400\" height=\"200\">\n",
        "   </details>\n",
        "\n",
        "<details>\n",
        "<summary>Click to view the control image autoencoder!</summary>\n",
        "\n",
        "<img  src=\"https://drive.google.com/uc?id=1sdql_R9KtWm3nkFY4G6D9Zzqzu2zp-8N\">\n",
        "\n",
        "<img  src=\"https://drive.google.com/uc?id=1XjcRJuNTI7B7AsZrNIg4bRJ7_yuqr4lV\" width=\"300\" height=\"400\">\n",
        "<img  src=\"https://drive.google.com/uc?id=1L-FxmT7JPv7IhhABZ3ceiI_f_5o8HK8f\" width=\"300\" height=\"400\">\n",
        "\n",
        "</details>\n",
        "\n",
        "</details>  \n",
        "\n",
        "<details>\n",
        "  <summary>Click to read about modeling the control data image classification! </summary>\n",
        "  \n",
        "  ## Modeling\n",
        "  To model our data effectively; we created a image classifier with few simple layers.\n",
        "\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=10FL_C81awujhuK9IfHnsxGxTEYqFcKiw\" >\n",
        "\n",
        "  * Conv2D \n",
        "\n",
        "    * Scans the image and takes each pixel value in a 3x3 (or 4x4, depends on setting) part and multiplies it by a certain weight, adds the numbers and uses that number as the value of the output image of that pixel\n",
        "    * Depending on the weights, the output image can be blurred, brighter, darker, etc. (basicallu, slight photoshop)\n",
        "\n",
        "  * MaxPooling2\n",
        "    * Resizes the output image\n",
        "    * Takes 2x2 area of a image and chooses the max value in each area\n",
        "    * Shrinks the image by a factor of two\n",
        "    * After shrinking usually the images are used for additional filters (Conv2D) in order to train on smaller scales to find more patterns\n",
        "  \n",
        "  * Flatten\n",
        "    * Because we will be using Dense next, we need to use Flatten to reduce the number of dimensions to one dimension\n",
        "  \n",
        "  * Dense\n",
        "    * Dense layers are hidden layers that use different acitvation functions to find the weights of each parameter of the image in order to appropriately learn the images\n",
        "\n",
        "  * Dropout\n",
        "    * Usually set to 40%, which means that 40% of the parameters that go into the Dropout are set to zero,\n",
        "    * This helps the model not overfit\n",
        "\n",
        "This leads us to having 3,940,000 differnt parameters that our model is using and going throught in order to train on the images.\n",
        "\n",
        "## Evaluating Classification\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=18k7DyV5mfu_aaZ_s4G6EaYuvUwx1ADY7\" width=\"500\" height=\"600\" >\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1EeRqBK6PLqbDEkd4Z6Zw3gdplpkQ4E5O\" width=\"500\" height=\"600\">\n",
        "  \n",
        "* Here we see that the training accuracy and loss is getting better with time but the spikes show that the models are not overfitting\n",
        " \n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1LFUbMEG7nBBZQb_vm6bKrbVvUYVznb7E\" width=\"400\" height=\"500\">\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1O4x9Q2m_tNNVq8C-4ERx8uwpXWf6a4Ye\" width=\"600\" height=\"300\">\n",
        "\n",
        "   * The image classification model shows that the simple control model is not that good at identifing the images.\n",
        "\n",
        "</details>  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXiyrDgIZ39B",
        "colab_type": "text"
      },
      "source": [
        "# Experimental\n",
        "\n",
        "<details>\n",
        "  <summary>Click to read about modeling the experimental data! </summary>\n",
        "  \n",
        "\n",
        "<img  src=\"https://drive.google.com/uc?id=1XjcRJuNTI7B7AsZrNIg4bRJ7_yuqr4lV\" width=\"300\" height=\"400\">\n",
        "<img  src=\"https://drive.google.com/uc?id=1L-FxmT7JPv7IhhABZ3ceiI_f_5o8HK8f\" width=\"300\" height=\"400\">\n",
        "\n",
        "* Using these augmented images that are blurred as our input into the image classification. Instead of the original (focused) images we are using the images recreated by our autoencoder.\n",
        "\n",
        "</details>  \n",
        "\n",
        "<details>\n",
        "  <summary>Click to read about modeling the experimental data for image classification! </summary>\n",
        "  \n",
        "  ## Modeling\n",
        "  To model our data effectively; we used the same image classifier throughout the experiment. \n",
        "\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=10FL_C81awujhuK9IfHnsxGxTEYqFcKiw\" >\n",
        "\n",
        "\n",
        "## Evaluating Classification\n",
        "  \n",
        "Here we see the training accuracy and loss to get a better idea of what is going on\n",
        " \n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1OdcLhcQWxD-Zy3oB3rkMPfMIOHcFH0Xu\" width=\"500\" height=\"600\">\n",
        "   <img style=\"float: center;\" src=\"https://drive.google.com/uc?id=1yJWFmObhYEHB6vYhErv0jylouqiQy4Dj\" width=\"500\" height=\"600\">\n",
        "\n",
        "   * As shown above the accuracy of the training data is steadily going up, if the number of epochs is increased would still slowly climb.\n",
        "   * The accuracy of the validation data is stagnant. This could mean that the model is overfitting for the triaining data that the new data is confusing it. \n",
        "   * The loss of the model is another indicator that in the current model it is overfitting. Menaing that that the loss of the training data is getting smaller but the validation data is growing due to the model not being familiar with it.\n",
        "\n",
        "The image classification model shows that the simple image classification with the augmented images does not improve the models ability of classifing the images.\n",
        "\n",
        "</details>  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4X0HQcDUly_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Interpert\n",
        "<details>\n",
        "  <summary>Click to read about interperting our results! </summary>\n",
        "  \n",
        "  ## Interpreting the results\n",
        "  * The way to interpret the results, is by reviewing the confusion matrices and classification report of both the images to see how well they had performed. \n",
        "\n",
        "<img style=\"float: left;\" src=\"https://drive.google.com/uc?id=1OZzWsICFEukRUo5evZSQg4aiZnKaHZXo\" >\n",
        "<img style=\"float: left;\" src=\"https://drive.google.com/uc?id=1LFUbMEG7nBBZQb_vm6bKrbVvUYVznb7E\" >\n",
        "\n",
        "  * The confusion matrices tells us how well the model performed on the test images by displaying what the model classified the images as based on the accurate labels of the images. \n",
        "  * As shown the confusion matrix of the augmented images, the majority of the images were classified as one type of image which drastically dropped the accuracy and reliability of the images. \n",
        "  * The original image classification seems to have had a more diverse outcome but still was fairly inaccurate.\n",
        "\n",
        "By observing all the accuracy, loss, and confusion matrices of the model, we are able to infer that augmented images are not capable of increasing the accuracy of the images due to the models overfitting. \n",
        "\n",
        "</details>  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HirPj-_UloX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Future Work\n",
        "<details>\n",
        "  <summary> Click to future work for image classification! </summary>\n",
        "  \n",
        "  ## Future work for experimenting with augmented images\n",
        "  \n",
        "  * Obtain more images of each category will give the model more refrences to learn from \n",
        "  * Create deeper models with more layer that can handle learning for longer periods of time and be more accurate\n",
        "  * Attempt to use the encoded part of an images to classify the images instead of the entire image classification on recreated augmented images.\n",
        "\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJd3-WluUofO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Final Project Submission\n",
        "\n",
        "* Student name: Alisa Radjabova\n",
        "* Student pace: Full Time\n",
        "* Instructor name: Rafael\n",
        "* Blog post URL: \n",
        "* Presentation: \n",
        "* Video : \n",
        "\n"
      ]
    }
  ]
}